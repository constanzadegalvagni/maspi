{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "4qmXoXa-vm6U",
        "outputId": "cee31508-1c8f-4021-cf38-e570e379278c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]]\n",
            "Iteracion 0: Error Total 0.127748\n",
            "Iteracion 1: Error Total 0.001366\n",
            "Iteracion 2: Error Total 0.001224\n",
            "Iteracion 3: Error Total 0.001110\n",
            "Iteracion 4: Error Total 0.001016\n",
            "Iteracion 5: Error Total 0.000938\n",
            "Iteracion 6: Error Total 0.000871\n",
            "Iteracion 7: Error Total 0.000814\n",
            "Iteracion 8: Error Total 0.000764\n",
            "Iteracion 9: Error Total 0.000720\n",
            "X: [1,1], -> T(0) Esperado: -1.000000 - Calculado: -0.999623\n",
            "X: [1,-1], -> T(1) Esperado: 1.000000 - Calculado: 0.994893\n",
            "X: [-1,1], -> T(2) Esperado: 1.000000 - Calculado: 0.949452\n",
            "X: [-1,-1], -> T(3) Esperado: -1.000000 - Calculado: -0.982759\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov  8 13:16:28 2021\n",
        "\n",
        "@author: pnegr\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## La funcion de activacion es tanh. Se devuelve tambien la derivada.\n",
        "def activation(x):\n",
        "    b = 2.5;\n",
        "    x = b*x\n",
        "    t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    dt=(1-t**2)\n",
        "    return t,b*dt\n",
        "\n",
        "# La entrada consiste en un array cuyas columnas son los ejemplos de\n",
        "# aprendizaje y cuyas lineas son los descriptores\n",
        "X = np.array([[1, 1, -1, -1],[ 1, -1, 1, -1]])\n",
        "[Ni, M]          = X.shape\n",
        "No\t\t         = 1\n",
        "Nh               = 2\n",
        "Theta            = 10e-3\n",
        "eta              = 0.25 # ratio de aprendizaje\n",
        "epochs           = 10 # son las iteraciones para el aprendizaje\n",
        "# Las salidas esperadas o targets\n",
        "T = np.array([-1, 1, 1, -1])\n",
        "\n",
        "# Se inicializa la red\n",
        "# Hay una sola neurona de salida, con lo cual entre las neuronas escondidas\n",
        "# y la salida hay un vector de pesos. Entre la entrada y las neuronas\n",
        "# escondidas hay una matriz de pesos.\n",
        "\n",
        "#print(Nh,Ni)\n",
        "# Hidden weights (NhxNi)\n",
        "Wh = np.array([[0.3615, -1.4145], [ -0.8916,  0.2010]]).T.reshape((Ni,Nh))\n",
        "# Output weights (NoxNh)\n",
        "Wo = np.array([-1.1678 ,-0.2166]).reshape((Nh,No))\n",
        "# Los bias se fijan a un valor igual a 1. No hay aprendizaje en ellos.\n",
        "bo = 1\n",
        "bh = 1\n",
        "\n",
        "print(X[:,0].reshape((Ni,1)))\n",
        "\n",
        "\n",
        "J       = np.zeros((epochs))\n",
        "J[0]    = 1e3\n",
        "m       = 0\n",
        "while m < epochs:\n",
        "    for i in range(M):\n",
        "        # voy a tomar uno a uno los puntos para actualizar los pesos\n",
        "        Xm = X[:,i].reshape(1,Ni)\n",
        "        tk = T[i] \n",
        "        # Forward propagation desde la entrada X\n",
        "        # Calcular primero las aj en las neuronas escondidas\n",
        "        aj\t\t\t\t=  np.matmul(Xm, Wh)\n",
        "        aj              = aj + bh * np.ones((1, Nh))\n",
        "        [y, dfh]\t\t= activation(aj) ##recordar que devuelve el resultado de la activaciÃ³n y la derivada\n",
        "        # Calcular ahora el valor de salida utilizando lo precedente\n",
        "        \n",
        "        ak\t\t\t\t=  np.matmul(y,Wo)\n",
        "        ak              = ak + bo\n",
        "        [zk, dfo]\t    = activation(ak)\n",
        "        # Ya se puede calcular la salida y el error cometido\n",
        "        # Evaluar ahora el delta_k a la salida:\n",
        "        delta_k\t\t=  (tk - zk) * dfo\n",
        "        #...y delta_j:\n",
        "        #.                     #1x2         #matriz por escalar\n",
        "        #print(Wo)\n",
        "        delta_j\t\t=  dfh.T * (Wo * delta_k)\n",
        "        #delta_j\t\t=  dfh.T * (Wo @ delta_k)\n",
        "        #print((eta * delta_k * y.T))\n",
        "        #% Ahora se actualizan los pesos\n",
        "        ## los pesos de la capa de salida\n",
        "        Wo\t\t\t\t=  Wo + (eta * delta_k * y.T)\n",
        "        #Wo = Wo + eta * (y.T @ delta_k)\n",
        "        ##% los pesos de la capa escondida\n",
        "        Wh\t\t\t\t=  Wh + (eta * (Xm.T @ delta_j.T))\n",
        "        #Wh = Wh + eta * (Xm.T @ delta_j)\n",
        "\n",
        "    #Calculate total error\n",
        "    J[m]    = 0;\n",
        "    for i in range(M):\n",
        "        Xm = X[:,i].reshape((1,Ni))\n",
        "        aj\t\t\t\t=  np.matmul(Xm, Wh)\n",
        "        aj              = aj + bh * np.ones((1, Nh))\n",
        "        [y, dfh]\t\t= activation(aj)\n",
        "        ak\t\t\t\t=  np.matmul(y,Wo)\n",
        "        ak              = ak + bo\n",
        "        [zk, dfo]\t    = activation(ak);\n",
        "        #print(T[i], zk)\n",
        "        J[m] = J[m] + np.power(T[i] - zk[0,0],2);\n",
        "\n",
        "    J[m] = J[m]/M;\n",
        "    print('Iteracion %d: Error Total %f' % (m, J[m]))\n",
        "    m = m + 1;\n",
        "\n",
        "# El error a la salida debe ser aproximadamente 0.00071965\n",
        "expErr = 0.00071965;\n",
        "assert(np.abs(J[epochs-1]-expErr) < 1e-6)#, 'Error de implementacion')\n",
        "\n",
        "# Resultado\n",
        "for i in range(M):\n",
        "    Xm = X[:,i].reshape((1, Ni))\n",
        "    [y, dfh]\t\t= activation(Xm @ Wh + bh*np.ones((1,Ni)))\n",
        "    [zk, dfo]\t= activation(y @ Wo + bo);\n",
        "\n",
        "    print('X: [%d,%d], -> T(%d) Esperado: %f - Calculado: %f' % (X[0,i], X[1,i], i, T[i], zk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array([[3,2]]).T\n",
        "\n",
        "y = np.array([[10]]).T\n",
        "y * 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
